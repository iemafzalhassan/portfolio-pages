<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Local AI on Afzal Hassan - DevOps Engineer</title><link>https://iemafzalhassan.me/portfolio-pages/tags/local-ai/</link><description>Recent content in Local AI on Afzal Hassan - DevOps Engineer</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© 2025 Afzal Hassan</copyright><lastBuildDate>Mon, 11 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://iemafzalhassan.me/portfolio-pages/tags/local-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Running Llama 3.2 Locally with OpenWebUI and Docker</title><link>https://iemafzalhassan.me/portfolio-pages/blogs/ollama/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://iemafzalhassan.me/portfolio-pages/blogs/ollama/</guid><description>&lt;p>If you&amp;rsquo;ve ever wanted to run a powerful AI language model like Llama 3.2 on your own computer—without worrying about sharing your data with companies or relying on cloud services—this guide will walk you through the entire process. We&amp;rsquo;ll cover how to download, install, and set up &lt;strong>OpenWebUI&lt;/strong> on your local machine using &lt;strong>Docker&lt;/strong> and integrate it with a locally installed &lt;strong>Llama 3.2 model&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h3 class="relative group">What You Will Need:
&lt;div id="what-you-will-need" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#what-you-will-need" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>A computer with at least 8GB of RAM&lt;/strong> (for smooth performance with the Llama models).&lt;/li>
&lt;li>&lt;strong>Docker&lt;/strong> installed on your computer.&lt;/li>
&lt;li>&lt;strong>A terminal&lt;/strong> for running commands.&lt;/li>
&lt;li>&lt;strong>A bit of patience&lt;/strong> as the installation process will take some time.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 class="relative group">Step 1: Download and Install &lt;strong>Ollama&lt;/strong> (LLama Model Assistant)
&lt;div id="step-1-download-and-install-ollama-llama-model-assistant" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#step-1-download-and-install-ollama-llama-model-assistant" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h3>
&lt;p>First, we need to install &lt;strong>Ollama&lt;/strong>, a tool that allows you to manage and run local AI models such as &lt;strong>Llama 3.2&lt;/strong>.&lt;/p></description></item></channel></rss>